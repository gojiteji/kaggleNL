{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepNx.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmaKRSW_m44C",
        "colab_type": "code",
        "outputId": "5acb5eff-2fbc-4941-8a92-307aa15cc2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mv kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 777 /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c nlp-getting-started"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/411k [00:00<?, ?B/s]\n",
            "100% 411k/411k [00:00<00:00, 60.9MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/965k [00:00<?, ?B/s]\n",
            "100% 965k/965k [00:00<00:00, 58.7MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/22.2k [00:00<?, ?B/s]\n",
            "100% 22.2k/22.2k [00:00<00:00, 22.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haiwJ7a0nEsE",
        "colab_type": "code",
        "outputId": "0445b3e2-7c68-4dcf-8ed4-95f4c377d5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize ,TweetTokenizer\n",
        "import torch.optim as optimizers\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "warnings.simplefilter('ignore')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "rus = RandomUnderSampler()\n",
        "#辞書構造\n",
        "class Vocabulary():\n",
        "    def __init__(self,disabletokenize=False):\n",
        "        self.w2i={}\n",
        "        self.i2w={}\n",
        "        self.oov_char='<unk>'\n",
        "        self.pad='<pad>'\n",
        "        self.bos='<bos>'\n",
        "        self.eos='<eos>'\n",
        "\n",
        "        self.special_chars = [self.pad,self.oov_char,self.bos,self.eos]\n",
        "        self.data = \"\"\n",
        "        self._words=set()\n",
        "        self.tokenize=disabletokenize\n",
        "        self.tknzr = TweetTokenizer()\n",
        "    def update(self,text):\n",
        "        if(self.tokenize):\n",
        "            self._words.update(text)\n",
        "        else:\n",
        "            self.data=self.tknzr.tokenize(text)\n",
        "            #self.data=word_tokenize(text)\n",
        "            self._words.update(self.data)\n",
        "        self.w2i = {w: (i + len(self.special_chars)) for i, w in enumerate(self._words)}\n",
        "\n",
        "        self.i2w = {i: w for w, i in self.w2i.items()}\n",
        "        self.w2i['<pad>'] = 0\n",
        "        self.i2w[0] = '<pad>'\n",
        "        self.w2i['<unk>'] = 1\n",
        "        self.i2w[1] = '<unk>'\n",
        "        self.w2i['<bos>'] = 2\n",
        "        self.i2w[2] = '<bos>'\n",
        "        self.w2i['<eos>'] = 3\n",
        "        self.i2w[3] = '<eos>'\n",
        "\n",
        "\n",
        "    def encode(self,words):\n",
        "        output=[]\n",
        "        if(self.tokenize):\n",
        "            pass\n",
        "        else:\n",
        "            #words=word_tokenize(words)\n",
        "            words=self.tknzr.tokenize(words)\n",
        "        for word in words:\n",
        "            #辞書になし\n",
        "            if word not in self.w2i:\n",
        "                index = self.w2i[self.oov_char]#既存の<unk>を返す\n",
        "            else:\n",
        "            #辞書にあり\n",
        "                index = self.w2i[word]#idを引っ張ってくる\n",
        "            output.append(index)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def decode(self,indexes):#使いどころないけど確認用\n",
        "        out=[]\n",
        "        for index in indexes:\n",
        "            out.append(self.i2w[index])\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "keyword_voc=Vocabulary(disabletokenize=True)\n",
        "keywords = np.array(df.iloc[:,[1]].fillna('nan'))\n",
        "keywords=keywords.reshape(len(keywords))\n",
        "keyword_voc.update(list(keywords))#キーワードを辞書に登録\n",
        "text_voc=Vocabulary()\n",
        "text = np.array(df.iloc[:,[3]])\n",
        "text=text.reshape(len(text))\n",
        "\n",
        "x=np.stack([text, keywords], 1)\n",
        "y = np.array(df.iloc[:,4])\n",
        "raw_X_train=x\n",
        "raw_y_train=y\n",
        "#raw_X_train, X_test, raw_y_train, y_test = train_test_split(x, y,test_size=0.3,shuffle=True,stratify=y)\n",
        "\n",
        "#trainデータを辞書に登録\n",
        "for t in raw_X_train:\n",
        "    text_voc.update(t[0])\n",
        "maxlen=158"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCQdLqpAnHJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self,d_k,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.scaler = np.sqrt(d_k)\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        '''\n",
        "        # Argument\n",
        "            q, k, v: (batch, sequence, out_features)\n",
        "            mask:    (batch, sequence)\n",
        "        '''\n",
        "        score = torch.einsum('ijk,ilk->ijl', (q, k)) / self.scaler\n",
        "        score = score - torch.max(score, dim=-1, keepdim=True)[0]\n",
        "        score = torch.exp(score)\n",
        "        if mask is not None:\n",
        "            if len(mask.size()) == 2:\n",
        "                mask = mask.unsqueeze(1).repeat(1, score.size(1), 1)\n",
        "            score.data.masked_fill_(mask, 0)\n",
        "        a = score / torch.sum(score, dim=-1, keepdim=True)\n",
        "        c = torch.einsum('ijk,ikl->ijl', (a, v))\n",
        "        return c\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, output_dim,maxlen=10000,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.output_dim = output_dim\n",
        "        self.maxlen = maxlen\n",
        "        pe = self.initializer()\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x, mask=None):\n",
        "        pe = self.pe[:x.size(1), :].unsqueeze(0)\n",
        "        return x + pe\n",
        "    def initializer(self):\n",
        "        pe = \\\n",
        "            np.array([[pos / np.power(100, 2 * (i // 2) / self.output_dim)\n",
        "                       for i in range(self.output_dim)]\n",
        "                      for pos in range(self.maxlen)])\n",
        "        pe[:, 0::2] = np.sin(pe[:, 0::2])\n",
        "        pe[:, 1::2] = np.cos(pe[:, 1::2])\n",
        "        return torch.from_numpy(pe).float()\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,h,d_model,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.h = h\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_k = d_model // h\n",
        "        self.d_v = d_v = d_model // h\n",
        "        self.device = device\n",
        "        self.W_q = nn.Parameter(torch.Tensor(h,d_model, d_k))\n",
        "        self.W_k = nn.Parameter(torch.Tensor(h,d_model,d_k))\n",
        "        self.W_v = nn.Parameter(torch.Tensor(h,d_model,d_v))\n",
        "\n",
        "        nn.init.xavier_normal_(self.W_q)\n",
        "        nn.init.xavier_normal_(self.W_k)\n",
        "        nn.init.xavier_normal_(self.W_v)\n",
        "\n",
        "        self.attn = ScaledDotProductAttention(d_k)\n",
        "        self.linear = nn.Linear((h * d_v), d_model)\n",
        "        nn.init.xavier_normal_(self.linear.weight)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        '''\n",
        "        # Argument\n",
        "            q, k, v: (batch, sequence, out_features)\n",
        "            mask:    (batch, sequence)\n",
        "        '''\n",
        "        batch_size = q.size(0)\n",
        "        q = torch.einsum('hijk,hkl->hijl',(q.unsqueeze(0).repeat(self.h, 1, 1, 1),self.W_q))\n",
        "        k = torch.einsum('hijk,hkl->hijl',(k.unsqueeze(0).repeat(self.h, 1, 1, 1),self.W_k))\n",
        "        v = torch.einsum('hijk,hkl->hijl',(v.unsqueeze(0).repeat(self.h, 1, 1, 1),self.W_v))\n",
        "        q = q.view(-1, q.size(-2), q.size(-1))\n",
        "        k = k.view(-1, k.size(-2), k.size(-1))\n",
        "        v = v.view(-1, v.size(-2), v.size(-1))\n",
        "\n",
        "        if mask is not None:\n",
        "            multiples = [self.h] + [1] * (len(mask.size()) - 1)\n",
        "            mask = mask.repeat(multiples)\n",
        "\n",
        "        c = self.attn(q, k, v, mask=mask)\n",
        "        c = torch.split(c, batch_size, dim=0)\n",
        "        c = torch.cat(c, dim=-1)\n",
        "\n",
        "        out = self.linear(c)\n",
        "\n",
        "        return out\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(d_model, d_ff)\n",
        "        self.a1 = nn.ReLU()\n",
        "        self.l2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        h = self.l1(x)\n",
        "        h = self.a1(h)\n",
        "        y = self.l2(h)\n",
        "        return y\n",
        "\n",
        "class Attentions(nn.Module):\n",
        "    def __init__(self,depth_source,N=6,h=8,d_model=512,d_ff=2048,p_dropout=0.1,maxlen=128,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.embedding = nn.Embedding(depth_source,d_model, padding_idx=0)\n",
        "        self.pe = PositionalEncoding(d_model, maxlen=maxlen)\n",
        "        self.encoder_layers = nn.ModuleList([AttentioLayer(h=h,d_model=d_model,d_ff=d_ff,p_dropout=p_dropout,maxlen=maxlen,device=device) for _ in range(N)])\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        y = self.pe(x)\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            y = encoder_layer(y, mask=mask)\n",
        "        return y\n",
        "class AttentioLayer(nn.Module):\n",
        "    def __init__(self,h=8,d_model=512,d_ff=2048,p_dropout=0.1,maxlen=128,device='cuda'):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(h, d_model)\n",
        "        self.dropout1 = nn.Dropout(p_dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ff = FFN(d_model, d_ff)\n",
        "        self.dropout2 = nn.Dropout(p_dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        h = self.attn(x, x, x, mask=mask)\n",
        "        h = self.dropout1(h)\n",
        "        h = self.norm1(x + h)\n",
        "        y = self.ff(h)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.norm2(h + y)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n",
        "class AttentionNN(nn.Module):\n",
        "    def __init__(self,depth_source,depth_key,N=6,h=8,d_model=512,d_key=224,d_ff=2048,d_off=200,p_dropout=0.3,maxlen=128,device='cuda'):\n",
        "        \"\"\"\n",
        "        (形態素数＝ベクトル数)\n",
        "        depth_source:全ツイートの形態素数\n",
        "        N=6:エンコーダ/デコーダのレイヤを重ねる数\n",
        "        h=8:Multi head attentionにける、attentionの数\n",
        "        d_model=512:埋め込み次元\n",
        "        d_model=512:キーの埋め込み次元\n",
        "        d_ff=2048:フィードフォワード(通常のNNの順伝播)\n",
        "        p_dropout=0.3:ドロップアウト率\n",
        "        maxlen=128:1文の含む最大形態素数\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.embedding_key = nn.Embedding(depth_key,d_key, padding_idx=0)\n",
        "        self.d_model=d_model\n",
        "        self.maxlen = maxlen\n",
        "        self.Attantions = Attentions(depth_source=33550,N=N,h=h,d_model=d_model,d_ff=d_ff,p_dropout=p_dropout,maxlen=maxlen,device=device)\n",
        "        \n",
        "\n",
        "\n",
        "        self.linear_out = nn.Linear(d_key+d_model*maxlen, 1)\n",
        "        self.activation_out = nn.Sigmoid()\n",
        "\n",
        "        nn.init.xavier_normal_(self.linear_out.weight)\n",
        "\n",
        "    def sequence_mask(self, x):\n",
        "        return x.eq(0)\n",
        "    def forward(self,source, key):\n",
        "        key=self.embedding_key(key)\n",
        "        mask_source = self.sequence_mask(source)\n",
        "        hs = self.Attantions(source, mask=mask_source)\n",
        "        a_out=hs.reshape(len(source),self.d_model*maxlen)\n",
        "        b_out =key\n",
        "        out = torch.cat([a_out,b_out],dim=1)\n",
        "        out = self.linear_out(out)\n",
        "        out = self.activation_out(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_jjbGL1nJko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=AttentionNN(depth_source=len(text_voc.i2w),depth_key=len(keyword_voc.i2w),N=12,h=6,d_model=180,d_key=224,d_ff=190,d_off=4000,p_dropout=0.2,maxlen=maxlen,device='cuda').to('cuda')\n",
        "optimizer = optimizers.Adam(model.parameters(),lr=0.00001,amsgrad=True)\n",
        "criterion = nn.BCELoss()\n",
        "new=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gvQvWGVnORY",
        "colab_type": "code",
        "outputId": "9704ffaf-671c-44ba-8256-e016540c72bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "if(new):\n",
        "    training_loss = []\n",
        "    trainaccs=[]\n",
        "    accs=[]\n",
        "    new=False\n",
        "#モデルの学習\n",
        "epochs=3000\n",
        "\n",
        "cm1=0\n",
        "cm2=0\n",
        "\n",
        "#テストデータをダウンサンプリング\n",
        "#x_test_=np.concatenate([x_test.reshape(len(x_test),maxlen), x_test_key.reshape(len(x_test),1)],1)\n",
        "#data=torch.LongTensor(x_test_.astype(np.float32)).to('cuda')\n",
        "a=0\n",
        "for epoch in range(epochs):\n",
        "    \"\"\"\n",
        "    【データローダの設定】\n",
        "    trainデータはダウンサンプリングされるので、そのまま使うと14%ほど、学習データが失われる。\n",
        "    したがって、、エポックごとにraw_trainからサンプリングしたものをデータローダに設定\n",
        "    \"\"\"\n",
        "    X_train, y_train = rus.fit_sample(raw_X_train.reshape(-1, 2) ,raw_y_train.reshape(-1, 1) )\n",
        "    maxlen=158\n",
        "    id_=0\n",
        "    x_train=np.array([[0]*maxlen]*len(X_train))\n",
        "    x_train_key=np.array([0]*len(X_train))\n",
        "    for t in X_train:\n",
        "        tmp=text_voc.encode(t[0])\n",
        "        tmp.insert(0,2)#bos\n",
        "        tmp.append(3)#eos\n",
        "        tmp2=keyword_voc.encode([t[1]])\n",
        "        for i in range(maxlen-len(tmp)):\n",
        "            tmp.append(0)\n",
        "        x_train[id_]=tmp\n",
        "        x_train_key[id_]=tmp2[0]\n",
        "        id_=id_+1\n",
        "    \"\"\"データ水増し\"\"\"\n",
        "    unks=0#unkの量\n",
        "    max_unks=(x_train.size - sum(sum(x_train==0)))*0.25#0以外の量\n",
        "    max_row=x_train.shape[0]\n",
        "    max_colum=x_train.shape[1]\n",
        "    unks=[]\n",
        "    while(len(unks)<max_unks):\n",
        "        row=int(np.random.rand()*max_row)\n",
        "        colum=int(np.random.rand()*max_colum)\n",
        "        if(not(x_train[row,colum] == 0)):#paddingじゃない\n",
        "            if(not(x_train[row,colum] == 2)):#bosじゃない\n",
        "                if(not(x_train[row,colum] == 3)):#eosじゃない\n",
        "                    if(not(  [row,colum] in unks)):#既に置き換えていなければ\n",
        "                        x_train[row,colum]=1\n",
        "                        unks.append([row,colum])\n",
        "    print(\"begin training\")\n",
        "    batch_size=len(x_train)// 2000\n",
        "    x_in=np.concatenate([x_train.reshape(len(x_train),maxlen), x_train_key.reshape(len(x_train),1)], 1)\n",
        "    x=torch.LongTensor(x_in.astype(np.float32)).to('cuda')\n",
        "    y=torch.tensor(y_train.reshape(len(y_train),1).astype(np.float32)).to('cuda')\n",
        "    dataset = TensorDataset(x,y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #send_line_notification(i)\n",
        "    runnning_loss = 0.0\n",
        "    predis1=np.array([])\n",
        "    predis2=np.array([])\n",
        "    teachers=np.array([])\n",
        "    model.train()\n",
        "    i=0\n",
        "    for x,y in dataloader:\n",
        "        i=i+1\n",
        "        model.zero_grad()\n",
        "        predi=model(x[:,:-1],x[:,-1])\n",
        "        loss = criterion(predi,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        runnning_loss += loss.item()\n",
        "        predis1=np.append(predis1,predi.detach().cpu().numpy())\n",
        "        teachers=np.append(teachers,y.cpu())\n",
        "    max_acc=0\n",
        "    acc=0\n",
        "    max_thread=50\n",
        "    bestans=[0]\n",
        "    if(True):#for thread in range(40,99,3):\n",
        "        thread=50\n",
        "        predis2=np.where(predis1 > thread/100, 1, 0)\n",
        "        acc = accuracy_score(teachers,predis2)\n",
        "        cm1=confusion_matrix(teachers, predis2)\n",
        "        if(True):#acc>max_acc):\n",
        "            max_acc=acc\n",
        "            max_thread=thread/100\n",
        "            bestans =predis2\n",
        "    if(epoch%1==0):\n",
        "        print(\"epoch:\",epoch)\n",
        "        print(\"loss:\",runnning_loss)\n",
        "        print(\"*train data acc:\",max_acc)\n",
        "        a=sum(bestans)\n",
        "    trainaccs.append(max_acc)\n",
        "    training_loss.append(runnning_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "begin training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-fe426c62e5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mrunnning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BP_NQWVI72E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_threshold=0.5\n",
        "df_vali = pd.read_csv('test.csv')\n",
        "keywords_vali = np.array(df_vali.iloc[:,[1]].fillna('nan'))\n",
        "keywords_vali=keywords_vali.reshape(len(keywords_vali))\n",
        "text_vali = np.array(df_vali.iloc[:,[3]])\n",
        "text_vali=text_vali.reshape(len(text_vali))\n",
        "\n",
        "x=np.stack([text_vali, keywords_vali], 1)\n",
        "\n",
        "#testデータをid化\n",
        "maxlen=158\n",
        "id_=0\n",
        "x_vali=np.array([[0]*maxlen]*len(x))\n",
        "x_vali_key=np.array([0]*len(x))\n",
        "for t in x:\n",
        "    tmp=text_voc.encode(t[0])\n",
        "    tmp2=keyword_voc.encode([t[1]])\n",
        "    for i in range(maxlen-len(tmp)):\n",
        "        tmp.append(0)\n",
        "    x_vali[id_]=tmp\n",
        "    x_vali_key[id_]=tmp2[0]\n",
        "    id_=id_+1\n",
        "x_vali_=np.concatenate([x_vali.reshape(len(x_vali),maxlen), x_vali_key.reshape(len(x_vali),1)],1)\n",
        "data=torch.LongTensor(x_vali_.astype(np.float32)).to('cuda')\n",
        "predis1=np.array([])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YSJvhBjI-TM",
        "colab_type": "code",
        "outputId": "e575eedd-841e-457a-e7dc-20d4cdff3c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "model.eval()\n",
        "for j in range(data.shape[0]):\n",
        "    y_pred=model(data[j:j+1,:-1],data[j:j+1,-1])\n",
        "    predis1=np.append(predis1,y_pred.detach().cpu().numpy())\n",
        "print(\"threshold is:\",max_threshold)\n",
        "predis2=np.where(predis1 > max_threshold, 1, 0)\n",
        "plt.hist(predis2)\n",
        "plt.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "threshold is: 0.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7s+hQKvcDJ8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNPJKRbyJlDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv = pd.read_csv('sample_submission.csv')\n",
        "csv['target']=np.array(predis2).reshape(len(predis2))\n",
        "csv.to_csv('submission.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}